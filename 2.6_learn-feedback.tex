Recent advancements in LLMs have revolutionized the AI landscape~\cite{touvron2023llama1, touvron2023llama, driess2023palme, openai2023gpt4}. However, because they are mostly trained on data scraped from the web LLMs sometimes produce undesired outcomes, including generating biased or harmful content~\cite{10.1145/3442188.3445922}. Recognizing the importance of aligning LLMs with human values,   has led to efforts in supervised fine-tuning (SFT) with ethical guidelines~\cite{alpaca}. While these efforts demonstrate the potential of integrating human feedback into training using reinforcement learning for user-tailored tasks~\cite{ouyang2022training, bai2022training}, training LLMs to reflect human values is quite challenging. 

In our work, we adopt the idea of training language models with feedback. However, rather than relying on a human fact-checker, we utilize a surrogate reward model (an entailment model) to stand in the place of a human fact checker, in order to fine-tune the summarizer to generate summaries that give evidence for fact-checking specific claims through Proximal Policy Optimization (PPO)~\cite{schulman2017proximal, zheng2023secrets}.