We introduce~\textbf{MetaSumPerceiver}, a summarization model designed to produce concise, informative summaries for claim fact-checking from complex multimodal datasets. Our model's flexible architecture can accommodate arbitrary numbers of documents and types of inputs, including documents, images, and claims by leveraging a perceiver-based architecture. In addition, we propose~\textbf{KG2Claim}, a text generation pipeline to produce the claims from the knowledge graphs. Our text generation approach can generate claims related to multimodal mult-document information.

We train our model using a novel reinforcement learning approach in order to generate summaries useful for verifying the truthfulness of claims. Our experimental assessments on the MOCHEG and our Multi-News-Fact-Checking datasets highlight~\textbf{MetaSumPerceiver}'s robust performance in claim verification tasks and demonstrate its effectiveness in real-world fact-checking scenarios. This contribution underscores ~\textbf{MetaSumPerceiver}'s potential to streamline fact-checking processes in today's multimodal information landscape. Moreover, we release the publicly accessible Multi-News-Fact-Checking dataset, aimed at assisting researchers in developing multi-document fact-checking methods.

Furthermore, we employ our text generation pipeline to produce claims in the NewsStories dataset. Our analysis of the generated claims reveals that more than 60\% are factual, drawing public interest in fact-checking. Additionally, testing these claims with~\textbf{MetaSumPerceiver} demonstrates that over 70\% of them are entailed with the news sources. According to the above experiment, the conclusion indicates that more than 60\% of the claims are entailment claims, and people wish to discern whether they are correct.