Presently, a significant portion of news circulating on social media platforms like Facebook, Reddit, and Twitter is characterized by disinformation and misinformation. This prevalence of false information can significantly misguide readers, leading them towards extreme viewpoints and fostering distrust in government actions. Notably, this phenomenon is particularly pronounced in the lead-up to elections, as witnessed during the 2016 US presidential election, where fake news proliferated across social media platforms, impacting public sentiment. Similarly, the Brexit referendum saw both "Leave" and "Remain" campaigns accused of spreading disinformation, particularly regarding economic consequences and immigration. Amidst this backdrop, reliable news sources, such as National Public Radio and The Wall Street Journal, are scarce, emphasizing the need for a robust fact-checking method. This method should not only accumulate knowledge from diverse articles covering the same story but also possess the capability to comprehend information from different modalities. Such a comprehensive approach is essential to effectively combat the spread of disinformation and misinformation in the age of social media.

Fact-checking claims on social media platforms poses a significant challenge due to the large volume of news claims constantly being posted without sufficient methods for verification~\cite{1_Aimeur2023_cb}. Research~\cite{2_borel2018state} indicates that manually verifying all aspects of a 200-word claim can require up to 4 hours of dedicated effort because human fact-checkers must find supporting evidence which could require reviewing multiple sources, including articles, images, videos, etc. Further, despite the exceptional capabilities of large language models (LLMs)~\cite{touvron2023llama} in natural language processing (NLP) tasks, they still struggle to understand events across documents. The limitation~\cite{3_Jakesch_2023, 4_Jakesch_2023_1,5_kreps_mccain_brundage_2022, 6_goldstein2023generative, 7_spitale2023ai} is particularly problematic in the context of fact-checking, where a comprehensive understanding of events is crucial for accurate assessment. Recognizing the potential for LLMs to inadvertently generate misleading information, we underscore the importance of developing models specifically tailored to handle the complexity of cross-document event comprehension. As disinformation~\cite{8_shu2017fake,9_doi:10.1126/science.aap9559} continues to proliferate, addressing this gap in LLM capabilities becomes imperative to ensure reliable fact-checking and information verification. To solve this issue in the fact-checking task, our method mainly focuses on generating claim-specific summaries to assist this task following the evidence retrieval, claim verification, and justification production.