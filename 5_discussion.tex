Given the societal importance of fact-checking applications, it is important that the limitations of our methods be explored. Our experimental results reveal that the surrogate entailment model often assigns truthfulness labels for entailment even when it struggles to fully grasp the relationship between the claim and the summary with evidence. This issue not only impacts the judgment of the claim label but also affects~\textbf{MetaSumPerceiver} during training. One potential solution is using a textual entailment model adept at managing this uncertainty or excluding such instances during training. Furthermore, the experimental outcomes from the~\textbf{KG2Claim} method reveal that 30\% of the generated claims are not entailed with the news sources. The challenge lies in the possibility that multimodal multi-document knowledge graphs might incorporate irrelevant information. A potential remedy is to diversify the set of edge labels in the knowledge graphs. We propose incorporating additional labels, such as the content label, ads label, and quote label. This approach would help prioritize which edges are more crucial for traversal. Lastly, Llama 2's claims in the Multi-News-Fact-Checking dataset have certain flaws. Our review suggests that neutral claims might mix consistent and conflicting details. Enhancing our data creation prompts or the prompts used in the second-stage claiming could boost Llama 2's understanding.

~\textbf{MetaSumPerceiver}, trained on English text and topics from the Multi-News benchmarks, may not perform well in other languages without retraining. Care should be taken to ensure the model is trained on data that closely aligns with the target domain of interest, if possible, to minimize errors. Finally, our model relies on identifying relevant and trusted source documents on which to perform summarization and checking. While this document-level retrieval task is orthogonal to our research, failure to retrieve relevant documents will affect the downstream performance of the fact-checking system. If irrelevant documents are used, even true claims might be wrongly challenged. Thus, approaches should confirm that events and entities in sourced documents are directly related, employing sophisticated methods.
