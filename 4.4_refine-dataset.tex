In section~\ref{subse:MNFCD}, we described how we generated the claims and labels comprising our Multi-News-Fact-checking dataset. In total, our dataset consists of 1,687,200 claims and labels. However, in some cases, Llama-2-70b misunderstands the summary and predicts the wrong label for the claims. To assess the quality of our dataset, we employ Llama-2-70b once again for a thorough validation of our dataset's claims and respective labels (entailment, neutral, and contradiction). We provide the prompt we use for this double-checking procedure in the appendices~\ref{ase:app_prompt_llama2} and show the prompted claims in Figure~\ref{fig:prompt_ex}.

In Table~\ref{label:prompt_accuracy}, we show the performance of Llama-2-70b at predicting the label produced from the first phase of our dataset. We show that Llama-2-70b exhibits strong performance on entailment claims (acc 78.3\%) and contradiction claims (acc 74.1\%). We observe that Llama-2-70b performs worse at distinguishing neutral claims, registering an accuracy of only 64.2\%. This is likely because the neutral category requires identifying that a specific piece of a claim is neither entailed or contradicted. Thus, this case is harder than either entailment or contradiction alone. We show examples of specific prompts and corresponding entailment, neutral, and contradictory claims.

Additionally, we discovered that most predictions from Llama-2-70b are similar to human predictions, especially in entailment and contradiction claims. To investigate this similarity further, we conducted a human test by randomly selecting 200 claims. The results for entailment claims revealed that 65\% of them had the same prediction as Llama-2-70b. For contradiction claims, 73\% of the sampled claims matched Llama-2-70b's predictions. Finally, in neutral claims, 62\% of the sampled claims had the same prediction as Llama-2-70b.

\begin{table}[]\large
\centering
\caption{Performance of claim verification in Multi-News-Fact-Checking dataset. We compare our method with other offline summarization models.}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|cccccccc}\hline
\textbf{Setting}      & \textbf{Accuracy (\%)} & \textbf{\begin{tabular}[c]{@{}c@{}}Precision (\%)\\ Entailment \end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Precision (\%)\\ Contradiction\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Precision (\%)\\ Neutral\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Recall (\%)\\ Entailment\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Recall (\%)\\ Contradiction\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Recall (\%)\\ Neutral\end{tabular}} \\
\hline
PEGASUS $\rightarrow$ DeBERTa V3 & 33.2 & 64.2& 14.7& 21.5& 37.3& 12.4& 11.9 \\
PEGASUS $\rightarrow$ Llama 2 & 39.5 & 37.4& 23.1& 42.8& 27.6& 24.3& 24.0\\ \hline
T5 large $\rightarrow$ DeBERTa V3 & 34.8 & 62.8& 17.5& 26.2& 33.0& 18.5& 18.2\\
T5 large $\rightarrow$ Llama 2 & 37.2 & 40.2& 32.8& \textbf{48.0}& 30.5& 26.4& 26.8\\ \hline
Our $\rightarrow$ DeBERTa V3 & 36.7 & \textbf{75.5} & 28.9 & 27.5 & 41.0 & 21.7 & \textbf{47.2} \\
Our (No RL) $\rightarrow$ Llama 2 & 42.6 & 41.0 & \textbf{53.7} & 34.6 & 54.8 & 37.8 & 29.6 \\ 
Our $\rightarrow$ Llama 2 & \textbf{45.6} & 49.2 & 48.7 & 33.6 & \textbf{56.9} & \textbf{44.1} & 28.4 \\ \hline
\end{tabular}}
\label{label:multi-news}
\end{table}