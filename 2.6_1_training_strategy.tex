~\citet{devlin2019bert} employs two key unsupervised training tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP), which collectively contribute to the model's ability to understand contextualized word representations and relationships between sentences. The MLM task involves randomly masking some words in a sentence and training the model to predict the masked words based on the context of the surrounding words. This helps BERT learn bidirectional contextual representations, capturing the meaning of words in the context of the entire sentence. The NSP task, on the other hand, involves predicting whether one sentence follows another in a document. This task encourages the model to understand the relationships and coherence between sentences, enabling BERT to grasp the broader context of a document. By combining these tasks during pre-training, BERT learns rich contextualized representations that make it highly effective for a wide range of natural language processing tasks.

To enhance performance,~\citet{clark2020electra} suggests replacing the pretraining task with the "Replaced Token Detection" (RTD) task, akin to generative adversarial networks (GANs). In the RTD task, the objective is to discern whether a given token is original or not. Results indicate that weight sharing outperforms the MLM task because the RTD task updates only the input token embedding in the discriminator. DeBERTa V3 in Figure~\ref{fig:deberta} integrates the DeBERTa model with the RTD task, enhancing relative position embedding and emphasizing absolute position embedding in the final layer. Comparative results demonstrate DeBERTa's superiority over BERT. Moreover, DeBERTa V3 retains the DeBERTa model with the RTD task, achieving the best performance in GLUE with an impressive 91.37\% average score according to experiments. In our work, we primarily employ DeBERTa V3 as our entailment model to help determine whether a claim is entailed by a summary or not.