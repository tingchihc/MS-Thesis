There is a pressing requirement for tools that can efficiently combine multimodal multi-document information and provide concise evidence summaries for fact-checkers. Currently, there is limited research exploring the integration of multimodal information, especially in the fact-checking task~\cite{9068414}. Existing research~\cite{Das_2023,doi:10.1177,berlinski_doyl} relying on summarization for fact-checking is ineffective because it fails to extract evidence from the sources. These approaches lack the ability to comprehend entire articles and produce specific evidence to substantiate their claims. Moreover, prior research~\cite{10.1371/journal.pone.0150989} has shown existing systems are unable to effectively handle multimodal data.

Multimedia event extraction~\cite{chen2021joint,li2020crossmedia,li2020gaia,li2021documentlevel,10.1145/3503161.3548132} offers a potential solution to integrate information from multimodal sources. It focuses on extracting events and their associated details from various modalities simultaneously. The primary goals include classifying events into pre-defined event types and identifying arguments for each event, grounded in text entities or image objects. This task is demanding due to the complementary nature of information from different modalities. While multimedia event extraction can identify events and entities across various modalities, it falls short as an effective solution for the fact-checking task. The rationale behind this is that humans find multimodal information to be less efficient as a representation for quickly comprehending content.

Multimodal summarization~\cite{khullar2020mast,liu2023long,111_inproceedings,7926698} offers a promising solution to the challenge of condensing evidence. By integrating information from various sources, such as text, images, videos, and audio, it enables the generation of summaries that enhance people's understanding of diverse content. This is a challenging task because each modality might contribute complementary information, e.g. bar chart image with other relevant facts mentioned in the text. Current methods~\cite{rebuffel2019hierarchical,puduppully_data--text_2021,wang-etal-2022-robust} typically generate intuitive summaries using multimodal information. However, our aim differs. We do not use intuitive summaries for fact-checking since they lack the specific details needed to verify events or entities. Our goal is to efficiently distill claim-specific evidence useful for fact-checking across various modalities.

According to the current limitations mentioned above, we are still missing the method of generating evidence for the fact-checking task. We employ multimodal summarization techniques to create a model that generates claim-specific evidence for the fact-checking task. Furthermore, we follow multimedia event extraction lead in constructing a method that harnesses the power of these approaches, aiming to create a comprehensive framework for handling diverse sources of information. We envision an approach that seamlessly integrates textual, visual, and potentially auditory information to furnish the claim-specific summary for the fact-checking task. This integration will enable a more robust and contextually rich understanding of the content, breaking down the barriers between different modalities.