@misc{clark2020electra,
      title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}, 
      author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
      year={2020},
      eprint={2003.10555},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@ARTICLE{1_Aimeur2023_cb,
  title    = "Fake news, disinformation and misinformation in social media: a
              review",
  author   = "A{\"\i}meur, Esma and Amri, Sabrine and Brassard, Gilles",
  abstract = "Online social networks (OSNs) are rapidly growing and have become
              a huge source of all kinds of global and local news for millions
              of users. However, OSNs are a double-edged sword. Although the
              great advantages they offer such as unlimited easy communication
              and instant news and information, they can also have many
              disadvantages and issues. One of their major challenging issues
              is the spread of fake news. Fake news identification is still a
              complex unresolved issue. Furthermore, fake news detection on
              OSNs presents unique characteristics and challenges that make
              finding a solution anything but trivial. On the other hand,
              artificial intelligence (AI) approaches are still incapable of
              overcoming this challenging problem. To make matters worse, AI
              techniques such as machine learning and deep learning are
              leveraged to deceive people by creating and disseminating fake
              content. Consequently, automatic fake news detection remains a
              huge challenge, primarily because the content is designed in a
              way to closely resemble the truth, and it is often hard to
              determine its veracity by AI alone without additional information
              from third parties. This work aims to provide a comprehensive and
              systematic review of fake news research as well as a fundamental
              review of existing approaches used to detect and prevent fake
              news from spreading via OSNs. We present the research problem and
              the existing challenges, discuss the state of the art in existing
              approaches for fake news detection, and point out the future
              research directions in tackling the challenges.",
  journal  = "Soc. Netw. Anal. Min.",
  volume   =  13,
  number   =  1,
  pages    = "30",
  month    =  feb,
  year     =  2023,
  keywords = "Disinformation; Fake news; Information disorder; Misinformation;
              Online deception; Online social networks",
  language = "en"
}

@article{2_borel2018state,
  title={The state of fact-checking in science journalism},
  author={Borel, B and Sheikh, K and Husain, F and Junger, A and Biba, E and Blum, D and Urcuioli, B},
  journal={Cambridge, MA: Massachusetts Institute of Technology Knight Science Journalism Program},
  year={2018}
}

@article{3_Jakesch_2023,
	doi = {10.1073/pnas.2208839120},
  
	url = {https://doi.org/10.1073%2Fpnas.2208839120},
  
	year = 2023,
	month = {mar},
  
	publisher = {Proceedings of the National Academy of Sciences},
  
	volume = {120},
  
	number = {11},
  
	author = {Maurice Jakesch and Jeffrey T. Hancock and Mor Naaman},
  
	title = {Human heuristics for {AI}-generated language are flawed},
  
	journal = {Proceedings of the National Academy of Sciences}
}

@inproceedings{4_Jakesch_2023_1,
	doi = {10.1145/3544548.3581196},
  
	url = {https://doi.org/10.1145%2F3544548.3581196},
  
	year = 2023,
	month = {apr},
  
	publisher = {{ACM}
},
  
	author = {Maurice Jakesch and Advait Bhat and Daniel Buschek and Lior Zalmanson and Mor Naaman},
  
	title = {Co-Writing with Opinionated Language Models Affects Users' Views},
  
	booktitle = {Proceedings of the 2023 {CHI} Conference on Human Factors in Computing Systems}
}

@article{5_kreps_mccain_brundage_2022, title={All the News That’s Fit to Fabricate: AI-Generated Text as a Tool of Media Misinformation}, volume={9}, DOI={10.1017/XPS.2020.37}, number={1}, journal={Journal of Experimental Political Science}, publisher={Cambridge University Press}, author={Kreps, Sarah and McCain, R. Miles and Brundage, Miles}, year={2022}, pages={104–117}}

@misc{6_goldstein2023generative,
      title={Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations}, 
      author={Josh A. Goldstein and Girish Sastry and Micah Musser and Renee DiResta and Matthew Gentzel and Katerina Sedova},
      year={2023},
      eprint={2301.04246},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{7_spitale2023ai,
      title={AI model GPT-3 (dis)informs us better than humans}, 
      author={Giovanni Spitale and Nikola Biller-Andorno and Federico Germani},
      year={2023},
      eprint={2301.11924},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{8_shu2017fake,
      title={Fake News Detection on Social Media: A Data Mining Perspective}, 
      author={Kai Shu and Amy Sliva and Suhang Wang and Jiliang Tang and Huan Liu},
      year={2017},
      eprint={1708.01967},
      archivePrefix={arXiv},
      primaryClass={cs.SI}
}

@article{
9_doi:10.1126/science.aap9559,
author = {Soroush Vosoughi  and Deb Roy  and Sinan Aral },
title = {The spread of true and false news online},
journal = {Science},
volume = {359},
number = {6380},
pages = {1146-1151},
year = {2018},
doi = {10.1126/science.aap9559},
URL = {https://www.science.org/doi/abs/10.1126/science.aap9559},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aap9559},
abstract = {There is worldwide concern over false news and the possibility that it can influence political, economic, and social well-being. To understand how false news spreads, Vosoughi et al. used a data set of rumor cascades on Twitter from 2006 to 2017. About 126,000 rumors were spread by ∼3 million people. False news reached more people than the truth; the top 1\% of false news cascades diffused to between 1000 and 100,000 people, whereas the truth rarely diffused to more than 1000 people. Falsehood also diffused faster than the truth. The degree of novelty and the emotional reactions of recipients may be responsible for the differences observed. Science, this issue p. 1146 A large-scale analysis of tweets reveals that false rumors spread further and faster than the truth. We investigated the differential diffusion of all of the verified true and false news stories distributed on Twitter from 2006 to 2017. The data comprise ~126,000 stories tweeted by ~3 million people more than 4.5 million times. We classified news as true or false using information from six independent fact-checking organizations that exhibited 95 to 98\% agreement on the classifications. Falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political news than for false news about terrorism, natural disasters, science, urban legends, or financial information. We found that false news was more novel than true news, which suggests that people were more likely to share novel information. Whereas false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust. Contrary to conventional wisdom, robots accelerated the spread of true and false news at the same rate, implying that false news spreads more than the truth because humans, not robots, are more likely to spread it.}}

@ARTICLE{9068414,
  author={Zhang, Chao and Yang, Zichao and He, Xiaodong and Deng, Li},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={Multimodal Intelligence: Representation Learning, Information Fusion, and Applications}, 
  year={2020},
  volume={14},
  number={3},
  pages={478-493},
  doi={10.1109/JSTSP.2020.2987728}}
@article{Das_2023,
	doi = {10.1016/j.ipm.2022.103219},
  
	url = {https://doi.org/10.1016%2Fj.ipm.2022.103219},
  
	year = 2023,
	month = {mar},
  
	publisher = {Elsevier {BV}
},
  
	volume = {60},
  
	number = {2},
  
	pages = {103219},
  
	author = {Anubrata Das and Houjiang Liu and Venelin Kovatchev and Matthew Lease},
  
	title = {The state of human-centered {NLP} technology for fact-checking},
  
	journal = {Information Processing \& Management}
}

@article{doi:10.1177,
author = {Andrea Ceron and Paride Carrara},
title ={Fact-checking, reputation, and political falsehoods in Italy and the United States},

journal = {New Media \& Society},
volume = {25},
number = {3},
pages = {540-558},
year = {2023},
doi = {10.1177/14614448211012377},

URL = { 
    
        https://doi.org/10.1177/14614448211012377
    
    

},
eprint = { 
    
        https://doi.org/10.1177/14614448211012377
    
    

}
,
    abstract = { This article develops a reputational theory of political falsehoods. Politicians are motivated by the desire to build a positive reputation, therefore, they will be more likely to deliver false statements (incurring the risk of being fact-checked) when the potential benefit outweighs the cost. This happens as new elections come closer, since the electoral benefit of falsehoods increases along with the probability of being checked too late (after the election day). Politicians are less likely to issue falsehoods in detailed statements and in scripted communication, since the reputational cost is higher because such falsehoods would be considered intentional. Conversely, the stronger trust that voters attribute to politicians on issues they own, allows politicians to lie on such topics. Statistical analysis of almost 8000 statements released by politicians and assessed by fact-checkers, in the United States and Italy (2007–2018), supports the hypotheses. The results hold irrespective of party affiliation. }
}

@article{berlinski_doyl, title={The Effects of Unsubstantiated Claims of Voter Fraud on Confidence in Elections}, volume={10}, DOI={10.1017/XPS.2021.18}, number={1}, journal={Journal of Experimental Political Science}, publisher={Cambridge University Press}, author={Berlinski, Nicolas and Doyle, Margaret and Guess, Andrew M. and Levy, Gabrielle and Lyons, Benjamin and Montgomery, Jacob M. and Nyhan, Brendan and Reifler, Jason}, year={2023}, pages={34–49}}

@article{10.1371/journal.pone.0150989,
    doi = {10.1371/journal.pone.0150989},
    author = {Zubiaga, Arkaitz AND Liakata, Maria AND Procter, Rob AND Wong Sak Hoi, Geraldine AND Tolmie, Peter},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Analysing How People Orient to and Spread Rumours in Social Media by Looking at Conversational Threads},
    year = {2016},
    month = {03},
    volume = {11},
    url = {https://doi.org/10.1371/journal.pone.0150989},
    pages = {1-29},
    abstract = {As breaking news unfolds people increasingly rely on social media to stay abreast of the latest updates. The use of social media in such situations comes with the caveat that new information being released piecemeal may encourage rumours, many of which remain unverified long after their point of release. Little is known, however, about the dynamics of the life cycle of a social media rumour. In this paper we present a methodology that has enabled us to collect, identify and annotate a dataset of 330 rumour threads (4,842 tweets) associated with 9 newsworthy events. We analyse this dataset to understand how users spread, support, or deny rumours that are later proven true or false, by distinguishing two levels of status in a rumour life cycle i.e., before and after its veracity status is resolved. The identification of rumours associated with each event, as well as the tweet that resolved each rumour as true or false, was performed by journalist members of the research team who tracked the events in real time. Our study shows that rumours that are ultimately proven true tend to be resolved faster than those that turn out to be false. Whilst one can readily see users denying rumours once they have been debunked, users appear to be less capable of distinguishing true from false rumours when their veracity remains in question. In fact, we show that the prevalent tendency for users is to support every unverified rumour. We also analyse the role of different types of users, finding that highly reputable users such as news organisations endeavour to post well-grounded statements, which appear to be certain and accompanied by evidence. Nevertheless, these often prove to be unverified pieces of information that give rise to false rumours. Our study reinforces the need for developing robust machine learning techniques that can provide assistance in real time for assessing the veracity of rumours. The findings of our study provide useful insights for achieving this aim.},
    number = {3},

}

@inproceedings{li2020gaia,
    title={GAIA: A Fine-grained Multimedia Knowledge Extraction System},
    author={Manling Li and Alireza Zareian and Ying Lin and Xiaoman Pan and Spencer Whitehead and Brian Chen and Bo Wu and Heng Ji and Shih-Fu Chang and Clare Voss and Dan Napierski and Marjorie Freedman},
    booktitle={Proceedings of The 58th Annual Meeting of the Association for Computational Linguistics},
    year={2020}
}

@misc{chen2021joint,
      title={Joint Multimedia Event Extraction from Video and Article}, 
      author={Brian Chen and Xudong Lin and Christopher Thomas and Manling Li and Shoya Yoshida and Lovish Chum and Heng Ji and Shih-Fu Chang},
      year={2021},
      eprint={2109.12776},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{li2020crossmedia,
      title={Cross-media Structured Common Space for Multimedia Event Extraction}, 
      author={Manling Li and Alireza Zareian and Qi Zeng and Spencer Whitehead and Di Lu and Heng Ji and Shih-Fu Chang},
      year={2020},
      eprint={2005.02472},
      archivePrefix={arXiv},
      primaryClass={cs.MM}
}

@inproceedings{10.1145/3503161.3548132,
author = {Liu, Jian and Chen, Yufeng and Xu, Jinan},
title = {Multimedia Event Extraction From News With a Unified Contrastive Learning Framework},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548132},
doi = {10.1145/3503161.3548132},
abstract = {Extracting events from news have seen many benefits in downstream applications. Today's event extraction (EE) systems, however, usually focus on a single modality --- either for text or image, and such methods suffer from incomplete information because a news document is typically presented in a multimedia format. In this paper, we propose a new method for multimedia EE by bridging the textual and visual modalities with a unified contrastive learning framework. Our central idea is to create a shared space for texts and images in order to improve their similar representation. This is accomplished by training on text-image pairs in general, and we demonstrate that it is possible to use this framework to boost learning for one modality by investigating the complementary of the other modality. On the benchmark dataset, our approach establishes a new state-of-the-art performance and shows a 3 percent improvement in F1. Furthermore, we demonstrate that it can achieve cutting-edge performance for visual EE even in a zero-shot scenario with no annotated data in the visual modality.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {1945–1953},
numpages = {9},
keywords = {image representation learning, multimedia event extraction, contrastive learning},
location = {Lisboa, Portugal},
series = {MM '22}
}

@misc{li2021documentlevel,
      title={Document-Level Event Argument Extraction by Conditional Generation}, 
      author={Sha Li and Heng Ji and Jiawei Han},
      year={2021},
      eprint={2104.05919},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{khullar2020mast,
      title={MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical Attention}, 
      author={Aman Khullar and Udit Arora},
      year={2020},
      eprint={2010.08021},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{liu2023long,
      title={Long Text and Multi-Table Summarization: Dataset and Method}, 
      author={Shuaiqi Liu and Jiannong Cao and Ruosong Yang and Zhiyuan Wen},
      year={2023},
      eprint={2302.03815},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{111_inproceedings,
author = {Rott, Michal and Červa, Petr},
year = {2016},
month = {09},
pages = {101-108},
title = {Speech-to-Text Summarization Using Automatic Phrase Extraction from Recognized Text},
volume = {9924},
isbn = {978-3-319-45509-9},
doi = {10.1007/978-3-319-45510-5_12}
}
@INPROCEEDINGS{7926698,
  author={Sah, Shagan and Kulhare, Sourabh and Gray, Allison and Venugopalan, Subhashini and Prud'Hommeaux, Emily and Ptucha, Raymond},
  booktitle={2017 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Semantic Text Summarization of Long Videos}, 
  year={2017},
  volume={},
  number={},
  pages={989-997},
  doi={10.1109/WACV.2017.115}}

@misc{rebuffel2019hierarchical,
      title={A Hierarchical Model for Data-to-Text Generation}, 
      author={Clément Rebuffel and Laure Soulier and Geoffrey Scoutheeten and Patrick Gallinari},
      year={2019},
      eprint={1912.10011},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{puduppully_data--text_2021,
	title = {Data-to-text {Generation} with {Macro} {Planning}},
	volume = {9},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl\_a\_00381},
	doi = {10.1162/tacl_a_00381},
	abstract = {Recent approaches to data-to-text generation have adopted the very successful encoder-decoder architecture or variants thereof. These models generate text that is fluent (but often imprecise) and perform quite poorly at selecting appropriate content and ordering it coherently. To overcome some of these issues, we propose a neural model with a macro planning stage followed by a generation stage reminiscent of traditional methods which embrace separate modules for planning and surface realization. Macro plans represent high level organization of important content such as entities, events, and their interactions; they are learned from data and given as input to the generator. Extensive experiments on two data-to-text benchmarks (RotoWire and MLB) show that our approach outperforms competitive baselines in terms of automatic and human evaluation.},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Puduppully, Ratish and Lapata, Mirella},
	month = may,
	year = {2021},
	note = {\_eprint: https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00381/1924176/tacl\_a\_00381.pdf},
	pages = {510--527},
}

@inproceedings{wang-etal-2022-robust,
    title = "Robust (Controlled) Table-to-Text Generation with Structure-Aware Equivariance Learning",
    author = "Wang, Fei  and
      Xu, Zhewei  and
      Szekely, Pedro  and
      Chen, Muhao",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.371",
    doi = "10.18653/v1/2022.naacl-main.371",
    pages = "5037--5048",
    abstract = "Controlled table-to-text generation seeks to generate natural language descriptions for highlighted subparts of a table. Previous SOTA systems still employ a sequence-to-sequence generation method, which merely captures the table as a linear structure and is brittle when table layouts change. We seek to go beyond this paradigm by (1) effectively expressing the relations of content pieces in the table, and (2) making our model robust to content-invariant structural transformations. Accordingly, we propose an equivariance learning framework, which encodes tables with a structure-aware self-attention mechanism. This prunes the full self-attention structure into an order-invariant graph attention that captures the connected graph structure of cells belonging to the same row or column, and it differentiates between relevant cells and irrelevant cells from the structural perspective. Our framework also modifies the positional encoding mechanism to preserve the relative position of tokens in the same cell but enforce position invariance among different cells. Our technology is free to be plugged into existing table-to-text generation models, and has improved T5-based models to offer better performance on ToTTo and HiTab. Moreover, on a harder version of ToTTo, we preserve promising performance, while previous SOTA systems, even with transformation-based data augmentation, have seen significant performance drops.",
}

@misc{otmazgin2022fcoref,
      title={F-coref: Fast, Accurate and Easy to Use Coreference Resolution}, 
      author={Shon Otmazgin and Arie Cattan and Yoav Goldberg},
      year={2022},
      eprint={2209.04280},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{cattan2021crossdocument,
      title={Cross-document Coreference Resolution over Predicted Mentions}, 
      author={Arie Cattan and Alon Eirew and Gabriel Stanovsky and Mandar Joshi and Ido Dagan},
      year={2021},
      eprint={2106.01210},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{cattan2021realistic,
      title={Realistic Evaluation Principles for Cross-document Coreference Resolution}, 
      author={Arie Cattan and Alon Eirew and Gabriel Stanovsky and Mandar Joshi and Ido Dagan},
      year={2021},
      eprint={2106.04192},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{inproceedings,
author = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
year = {2005},
month = {01},
pages = {177-190},
title = {The PASCAL recognising textual entailment challenge},
isbn = {978-3-540-33427-9},
doi = {10.1007/11736790_9}
}
@misc{poliak2020survey,
      title={A Survey on Recognizing Textual Entailment as an NLP Evaluation}, 
      author={Adam Poliak},
      year={2020},
      eprint={2010.03061},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Yao_2023,
	doi = {10.1145/3539618.3591879},
  
	url = {https://doi.org/10.1145%2F3539618.3591879},
  
	year = 2023,
	month = {jul},
  
	publisher = {{ACM}
},
  
	author = {Barry Menglong Yao and Aditya Shah and Lichao Sun and Jin-Hee Cho and Lifu Huang},
  
	title = {End-to-End Multimodal Fact-Checking and Explanation Generation: A Challenging Dataset and Models},
  
	booktitle = {Proceedings of the 46th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval}
}

@misc{alex2019multinews,
    title={Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model},
    author={Alexander R. Fabbri and Irene Li and Tianwei She and Suyi Li and Dragomir R. Radev},
    year={2019},
    eprint={1906.01749},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{bowman-etal-2015-large,
    title = "A large annotated corpus for learning natural language inference",
    author = "Bowman, Samuel R.  and
      Angeli, Gabor  and
      Potts, Christopher  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1075",
    doi = "10.18653/v1/D15-1075",
    pages = "632--642",
}

@misc{tan2022newsstories,
      title={NewsStories: Illustrating articles with visual summaries}, 
      author={Reuben Tan and Bryan A. Plummer and Kate Saenko and JP Lewis and Avneesh Sud and Thomas Leung},
      year={2022},
      eprint={2207.13061},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{jaegle2022perceiver,
      title={Perceiver IO: A General Architecture for Structured Inputs \& Outputs}, 
      author={Andrew Jaegle and Sebastian Borgeaud and Jean-Baptiste Alayrac and Carl Doersch and Catalin Ionescu and David Ding and Skanda Koppula and Daniel Zoran and Andrew Brock and Evan Shelhamer and Olivier Hénaff and Matthew M. Botvinick and Andrew Zisserman and Oriol Vinyals and Joāo Carreira},
      year={2022},
      eprint={2107.14795},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{jaegle2022perceiver_1,
      title={Perceiver IO: A General Architecture for Structured Inputs \& Outputs}, 
      author={Andrew Jaegle and Sebastian Borgeaud and Jean-Baptiste Alayrac and Carl Doersch and Catalin Ionescu and David Ding and Skanda Koppula and Daniel Zoran and Andrew Brock and Evan Shelhamer and Olivier Hénaff and Matthew M. Botvinick and Andrew Zisserman and Oriol Vinyals and Joāo Carreira},
      year={2022},
      eprint={2107.14795},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lewis2019bart,
      title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}, 
      author={Mike Lewis and Yinhan Liu and Naman Goyal and Marjan Ghazvininejad and Abdelrahman Mohamed and Omer Levy and Ves Stoyanov and Luke Zettlemoyer},
      year={2019},
      eprint={1910.13461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{garg2018stochastic,
      title={Stochastic Learning of Nonstationary Kernels for Natural Language Modeling}, 
      author={Sahil Garg and Greg Ver Steeg and Aram Galstyan},
      year={2018},
      eprint={1801.03911},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ballesteros2017amr,
      title={AMR Parsing using Stack-LSTMs}, 
      author={Miguel Ballesteros and Yaser Al-Onaizan},
      year={2017},
      eprint={1707.07755},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lyu2018amr,
      title={AMR Parsing as Graph Prediction with Latent Alignment}, 
      author={Chunchuan Lyu and Ivan Titov},
      year={2018},
      eprint={1805.05286},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zheng2023survey,
      title={A Survey of Document-Level Information Extraction}, 
      author={Hanwen Zheng and Sijia Wang and Lifu Huang},
      year={2023},
      eprint={2309.13249},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@INPROCEEDINGS{10221008,
  author={Ariyanto, Amelia Devi Putri and Fatichah, Chastine and Purwitasari, Diana},
  booktitle={2023 International Seminar on Intelligent Technology and Its Applications (ISITIA)}, 
  title={Semantic Role Labeling for Information Extraction on Indonesian Texts: A Literature Review}, 
  year={2023},
  volume={},
  number={},
  pages={119-124},
  doi={10.1109/ISITIA59021.2023.10221008}}

@misc{joshi2020spanbert,
      title={SpanBERT: Improving Pre-training by Representing and Predicting Spans}, 
      author={Mandar Joshi and Danqi Chen and Yinhan Liu and Daniel S. Weld and Luke Zettlemoyer and Omer Levy},
      year={2020},
      eprint={1907.10529},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{yao-etal-2023-learning,
    title = "Learning Event-aware Measures for Event Coreference Resolution",
    author = "Yao, Yao  and
      Li, Zuchao  and
      Zhao, Hai",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.855",
    doi = "10.18653/v1/2023.findings-acl.855",
    pages = "13542--13556",
    abstract = "Researchers are witnessing knowledge-inspired natural language processing shifts the focus from entity-level to event-level, whereas event coreference resolution is one of the core challenges. This paper proposes a novel model for within-document event coreference resolution. On the basis of event but not entity as before, our model learns and integrates multiple representations from both event alone and event pair. For the former, we introduce multiple linguistics-motivated event alone features for more discriminative event representations. For the latter, we consider multiple similarity measures to capture the distinction of event pair. Our proposed model achieves new state-of-the-art on the ACE 2005 benchmark, demonstrating the effectiveness of our proposed framework.",
}

@inproceedings{caciularu-etal-2021-cdlm-cross,
    title = "{CDLM}: Cross-Document Language Modeling",
    author = "Caciularu, Avi  and
      Cohan, Arman  and
      Beltagy, Iz  and
      Peters, Matthew  and
      Cattan, Arie  and
      Dagan, Ido",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.225",
    doi = "10.18653/v1/2021.findings-emnlp.225",
    pages = "2648--2662",
    abstract = "We introduce a new pretraining approach geared for multi-document language modeling, incorporating two key ideas into the masked language modeling self-supervised objective. First, instead of considering documents in isolation, we pretrain over sets of multiple related documents, encouraging the model to learn cross-document relationships. Second, we improve over recent long-range transformers by introducing dynamic global attention that has access to the entire input to predict masked tokens. We release CDLM (Cross-Document Language Model), a new general language model for multi-document setting that can be easily applied to downstream tasks. Our extensive analysis shows that both ideas are essential for the success of CDLM, and work in synergy to set new state-of-the-art results for several multi-text tasks.",
}

@misc{grenander2023sentenceincremental,
      title={Sentence-Incremental Neural Coreference Resolution}, 
      author={Matt Grenander and Shay B. Cohen and Mark Steedman},
      year={2023},
      eprint={2305.16947},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{10.1162/tacl_a_00543,
    author = {Bohnet, Bernd and Alberti, Chris and Collins, Michael},
    title = "{Coreference Resolution through a seq2seq Transition-Based System}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {11},
    pages = {212-226},
    year = {2023},
    month = {03},
    abstract = "{Most recent coreference resolution systems use search algorithms over possible spans to identify mentions and resolve coreference. We instead present a coreference resolution system that uses a text-to-text (seq2seq) paradigm to predict mentions and links jointly. We implement the coreference system as a transition system and use multilingual T5 as an underlying language model. We obtain state-of-the-art accuracy on the CoNLL-2012 datasets with 83.3 F1-score for English (a 2.3 higher F1-score than previous work [Dobrovolskii, 2021]) using only CoNLL data for training, 68.5 F1-score for Arabic (+4.1 higher than previous work), and 74.3 F1-score for Chinese (+5.3). In addition we use the SemEval-2010 data sets for experiments in the zero-shot setting, a few-shot setting, and supervised setting using all available training data. We obtain substantially higher zero-shot F1-scores for 3 out of 4 languages than previous approaches and significantly exceed previous supervised state-of-the-art results for all five tested languages. We provide the code and models as open source.1}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00543},
    url = {https://doi.org/10.1162/tacl\_a\_00543},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00543/2074888/tacl\_a\_00543.pdf},
}

@inproceedings{10.1145/3539597.3573038,
author = {Yong, Shan Jie and Dong, Kuicai and Sun, Aixin},
title = {DOCoR: Document-Level OpenIE with Coreference Resolution},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3573038},
doi = {10.1145/3539597.3573038},
abstract = {Open Information Extraction (OpenIE) extracts relational fact tuples in the form of <subject, relation,="" object=""> from text. Most existing OpenIE solutions operate at sentence level and extract relational tuples solely from a sentence. However, many sentences exist as a part of paragraph or a document, where coreferencing is common. In this demonstration, we present a system which refines the semantic tuples generated by OpenIE with the aid of a coreference resolution tool. Specifically, all coreferential mentions across the entire document are identified and grouped into coreferential clusters. Objects and subjects in the extracted tuples from OpenIE which match any coreferential mentions are then resolved with a suitable representative term. In this way, our system is able to resolve both anaphoric and cataphoric references, to achieve Document-level OpenIE with Coreference Resolution (DOCoR). The demonstration video can be viewed at https://youtu.be/o9ZSWCBvlDs},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {1204–1207},
numpages = {4},
keywords = {coreference resolution, knowledge graph construction},
location = {Singapore, Singapore},
series = {WSDM '23}
}</subject,>

@inproceedings{10.1007/978-3-031-40286-9_34,
author = {Theptakob, Nathanon and Seneewong Na Ayutthaya, Thititorn and Saetia, Chanatip and Chalothorn, Tawunrat and Buabthong, Pakpoom},
title = {A Cross-Document Coreference Resolution Approach to Low-Resource Languages},
year = {2023},
isbn = {978-3-031-40285-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-40286-9_34},
doi = {10.1007/978-3-031-40286-9_34},
abstract = {Coreference resolution is an important area of research in natural language processing that deals with the task of identifying and grouping all the expressions in a text that refer to the same entity. This work presents a system to improve and develop a coreference resolution model for Thai language, based on the existing English clustering-based model. Specifically, we introduce a method to convert Thai text into ECB + -equivalent datasets, which can be used as benchmark for the Thai language. This paper follows an existing model trained for English coreference resolution which uses agglomerative clustering to segment clusters of coreference entities across document. The model trained and evaluated using our data achieves the best CoNLL F1 score of 72.87. Finally, we present a comparative study of the effect of manual and automatic span extractors on Thai language model performance. The results of our study indicate that our proposed pipeline, which utilizes the fine-tuned longformer model as the encoder, offers a viable alternative to more complex and resource-intensive methods. Our work also suggests that the use of existing NER and entity recognizer models can help automate span annotation prior to the subsequent conference clustering module. This study offers a potential framework for the construction of coreference resolution models in other low-resource languages.},
booktitle = {Knowledge Science, Engineering and Management: 16th International Conference, KSEM 2023, Guangzhou, China, August 16–18, 2023, Proceedings, Part II},
pages = {422–431},
numpages = {10},
keywords = {Cross-document relation extraction, Knowledge graph construction, Clustering, Coreference resolution, Entity alignment},
location = {Guangzhou, China}
}

@ARTICLE{Liu2023il,
  title     = "A brief survey on recent advances in coreference resolution",
  author    = "Liu, Ruicheng and Mao, Rui and Luu, Anh Tuan and Cambria, Erik",
  journal   = "Artif. Intell. Rev.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  56,
  number    =  12,
  pages     = "14439--14481",
  month     =  dec,
  year      =  2023,
  copyright = "https://www.springernature.com/gp/researchers/text-and-data-mining",
  language  = "en"
}

@inproceedings{NIPS2013_1cecc7a7,
 author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Translating Embeddings for Modeling Multi-relational Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},
 volume = {26},
 year = {2013}
}

@inproceedings{10.5555/2893873.2894046,
author = {Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
title = {Knowledge Graph Embedding by Translating on Hyperplanes},
year = {2014},
publisher = {AAAI Press},
abstract = {We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {1112–1119},
numpages = {8},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}

@misc{mikolov2013distributed,
      title={Distributed Representations of Words and Phrases and their Compositionality}, 
      author={Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1310.4546},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{SHEN2022109597,
title = {A comprehensive overview of knowledge graph completion},
journal = {Knowledge-Based Systems},
volume = {255},
pages = {109597},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109597},
url = {https://www.sciencedirect.com/science/article/pii/S095070512200805X},
author = {Tong Shen and Fu Zhang and Jingwei Cheng},
keywords = {Knowledge Graph Completion (KGC), Classification, Comparisons and analyses, Performance evaluation, Overview},
abstract = {Knowledge Graph (KG) provides high-quality structured knowledge for various downstream knowledge-aware tasks (such as recommendation and intelligent question-answering) with its unique advantages of representing and managing massive knowledge. The quality and completeness of KGs largely determine the effectiveness of the downstream tasks. But in view of the incomplete characteristics of KGs, there is still a large amount of valuable knowledge is missing from the KGs. Therefore, it is necessary to improve the existing KGs to supplement the missed knowledge. Knowledge Graph Completion (KGC) is one of the popular technologies for knowledge supplement. Accordingly, there has a growing concern over the KGC technologies. Recently, there have been lots of studies focusing on the KGC field. To investigate and serve as a helpful resource for researchers to grasp the main ideas and results of KGC studies, and further highlight ongoing research in KGC, in this paper, we provide a all-round up-to-date overview of the current state-of-the-art in KGC. According to the information sources used in KGC methods, we divide the existing KGC methods into two main categories: the KGC methods relying on structural information and the KGC methods using other additional information. Further, each category is subdivided into different granularity for summarizing and comparing them. Besides, the other KGC methods for KGs of special fields (including temporal KGC, commonsense KGC, and hyper-relational KGC) are also introduced. In particular, we discuss comparisons and analyses for each category in our overview. Finally, some discussions and directions for future research are provided.}
}

@article{Lin_Liu_Sun_Liu_Zhu_2015, title={Learning Entity and Relation Embeddings for Knowledge Graph Completion}, volume={29}, url={https://ojs.aaai.org/index.php/AAAI/article/view/9491}, DOI={10.1609/aaai.v29i1.9491}, abstractNote={ &lt;p&gt; Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan}, year={2015}, month={Feb.} }

@article{Shi_Weninger_2018, title={Open-World Knowledge Graph Completion}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11535}, DOI={10.1609/aaai.v32i1.11535}, abstractNote={ &lt;p&gt; Knowledge Graphs (KGs) have been applied to many tasks including Web search, link prediction, recommendation, natural language processing, and entity linking. However, most KGs are far from complete and are growing at a rapid pace. To address these problems, Knowledge Graph Completion (KGC) has been proposed to improve KGs by filling in its missing connections. Unlike existing methods which hold a closed-world assumption, i.e., where KGs are fixed and new entities cannot be easily added, in the present work we relax this assumption and propose a new open-world KGC task. As a first attempt to solve this task we introduce an open-world KGC model called ConMask. This model learns embeddings of the entity’s name and parts of its text-description to connect unseen entities to the KG. To mitigate the presence of noisy text descriptions, ConMask uses a relationship-dependent content masking to extract relevant snippets and then trains a fully convolutional neural network to fuse the extracted snippets with entities in the KG. Experiments on large data sets, both old and new, show that ConMask performs well in the open-world KGC task and even outperforms existing KGC models on the standard closed-world KGC task. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Shi, Baoxu and Weninger, Tim}, year={2018}, month={Apr.} }

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@misc{yao2019kgbert,
      title={KG-BERT: BERT for Knowledge Graph Completion}, 
      author={Liang Yao and Chengsheng Mao and Yuan Luo},
      year={2019},
      eprint={1909.03193},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jaegle2021perceiver,
      title={Perceiver: General Perception with Iterative Attention}, 
      author={Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman and Oriol Vinyals and Joao Carreira},
      year={2021},
      eprint={2103.03206},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2019glue,
      title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
      author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2019},
      eprint={1804.07461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{Li_2023_CVPR,
    author    = {Li, Hao and Zhu, Jinguo and Jiang, Xiaohu and Zhu, Xizhou and Li, Hongsheng and Yuan, Chun and Wang, Xiaohua and Qiao, Yu and Wang, Xiaogang and Wang, Wenhai and Dai, Jifeng},
    title     = {Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {2691-2700}
}

@inproceedings{Butler:ECCV:2012,
title = {A naturalistic open source movie for optical flow evaluation},
author = {Butler, D. J. and Wulff, J. and Stanley, G. B. and Black, M. J.},
booktitle = {European Conf. on Computer Vision (ECCV)},
editor = {{A. Fitzgibbon et al. (Eds.)}},
publisher = {Springer-Verlag},
series = {Part IV, LNCS 7577},
month = oct,
pages = {611--625},
year = {2012}
}

@article{article,
author = {Junnan, Zhu and Zhou, Yu and Zhang, Jiajun and Li, Haoran and Zong, Chengqing and Li, Changliang},
year = {2020},
month = {04},
pages = {9749-9756},
title = {Multimodal Summarization with Guidance of Multimodal Reference},
volume = {34},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i05.6525}
}



@inproceedings{palaskar-etal-2019-multimodal,
    title = "Multimodal Abstractive Summarization for How2 Videos",
    author = "Palaskar, Shruti  and
      Libovick{\'y}, Jind{\v{r}}ich  and
      Gella, Spandana  and
      Metze, Florian",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1659",
    doi = "10.18653/v1/P19-1659",
    pages = "6587--6596",
    abstract = "In this paper, we study abstractive summarization for open-domain videos. Unlike the traditional text news summarization, the goal is less to {``}compress{''} text information but rather to provide a fluent textual summary of information that has been collected and fused from different source modalities, in our case video and audio transcripts (or text). We show how a multi-source sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, compare various models trained with different modalities and present pilot experiments on the \textit{How2 corpus} of instructional videos. We also propose a new evaluation metric (Content F1) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries, which is covered by metrics like ROUGE and BLEU.",
}

@inproceedings{zhu-etal-2018-msmo,
    title = "{MSMO}: Multimodal Summarization with Multimodal Output",
    author = "Zhu, Junnan  and
      Li, Haoran  and
      Liu, Tianshang  and
      Zhou, Yu  and
      Zhang, Jiajun  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1448",
    doi = "10.18653/v1/D18-1448",
    pages = "4154--4164",
    abstract = "Multimodal summarization has drawn much attention due to the rapid growth of multimedia data. The output of the current multimodal summarization systems is usually represented in texts. However, we have found through experiments that multimodal output can significantly improve user satisfaction for informativeness of summaries. In this paper, we propose a novel task, multimodal summarization with multimodal output (MSMO). To handle this task, we first collect a large-scale dataset for MSMO research. We then propose a multimodal attention model to jointly generate text and select the most relevant image from the multimodal input. Finally, to evaluate multimodal outputs, we construct a novel multimodal automatic evaluation (MMAE) method which considers both intra-modality salience and inter-modality relevance. The experimental results show the effectiveness of MMAE.",
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{driess2023palme,
      title={PaLM-E: An Embodied Multimodal Language Model}, 
      author={Danny Driess and Fei Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Pete Florence},
      year={2023},
      eprint={2303.03378},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{10.1145/3442188.3445922,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@misc{touvron2023llama1,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{bai2022training,
      title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}, 
      author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
      year={2022},
      eprint={2204.05862},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{schulman2017proximal,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{he2023debertav3,
      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, 
      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
      year={2023},
      eprint={2111.09543},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zheng2023secrets,
      title={Secrets of RLHF in Large Language Models Part I: PPO}, 
      author={Rui Zheng and Shihan Dou and Songyang Gao and Yuan Hua and Wei Shen and Binghai Wang and Yan Liu and Senjie Jin and Qin Liu and Yuhao Zhou and Limao Xiong and Lu Chen and Zhiheng Xi and Nuo Xu and Wenbin Lai and Minghao Zhu and Cheng Chang and Zhangyue Yin and Rongxiang Weng and Wensen Cheng and Haoran Huang and Tianxiang Sun and Hang Yan and Tao Gui and Qi Zhang and Xipeng Qiu and Xuanjing Huang},
      year={2023},
      eprint={2307.04964},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2021pretrain,
      title={Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing}, 
      author={Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
      year={2021},
      eprint={2107.13586},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{sanh2022multitask,
      title={Multitask Prompted Training Enables Zero-Shot Task Generalization}, 
      author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Tali Bers and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M. Rush},
      year={2022},
      eprint={2110.08207},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{dang2022prompt,
      title={How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models}, 
      author={Hai Dang and Lukas Mecke and Florian Lehmann and Sven Goller and Daniel Buschek},
      year={2022},
      eprint={2209.01390},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}
@misc{white2023prompt,
      title={A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT}, 
      author={Jules White and Quchen Fu and Sam Hays and Michael Sandborn and Carlos Olea and Henry Gilbert and Ashraf Elnashar and Jesse Spencer-Smith and Douglas C. Schmidt},
      year={2023},
      eprint={2302.11382},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@misc{xing2023prompt,
      title={Prompt Sapper: LLM-Empowered Software Engineering Infrastructure for AI-Native Services}, 
      author={Zhenchang Xing and Qing Huang and Yu Cheng and Liming Zhu and Qinghua Lu and Xiwei Xu},
      year={2023},
      eprint={2306.02230},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{lian2023llmgrounded,
      title={LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models}, 
      author={Long Lian and Boyi Li and Adam Yala and Trevor Darrell},
      year={2023},
      eprint={2305.13655},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{10.1162/tacl_a_00454,
    author = {Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
    title = "{A Survey on Automated Fact-Checking}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {178-206},
    year = {2022},
    month = {02},
    abstract = "{Fact-checking has become increasingly important due to the speed with which both
                    information and misinformation can spread in the modern media ecosystem.
                    Therefore, researchers have been exploring how fact-checking can be automated,
                    using techniques based on natural language processing, machine learning,
                    knowledge representation, and databases to automatically predict the veracity of
                    claims. In this paper, we survey automated fact-checking stemming from natural
                    language processing, and discuss its connections to related tasks and
                    disciplines. In this process, we present an overview of existing datasets and
                    models, aiming to unify the various definitions given and identify common
                    concepts. Finally, we highlight challenges for future research.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00454},
    url = {https://doi.org/10.1162/tacl\_a\_00454},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00454/1987018/tacl\_a\_00454.pdf},
}

@inproceedings{kirstain-etal-2021-coreference,
    title = "Coreference Resolution without Span Representations",
    author = "Kirstain, Yuval  and
      Ram, Ori  and
      Levy, Omer",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.3",
    doi = "10.18653/v1/2021.acl-short.3",
    pages = "14--19",
    abstract = "The introduction of pretrained language models has reduced many complex task-specific NLP models to simple lightweight layers. An exception to this trend is coreference resolution, where a sophisticated task-specific model is appended to a pretrained transformer encoder. While highly effective, the model has a very large memory footprint {--} primarily due to dynamically-constructed span and span-pair representations {--} which hinders the processing of complete documents and the ability to train on multiple instances in a single batch. We introduce a lightweight end-to-end coreference model that removes the dependency on span representations, handcrafted features, and heuristics. Our model performs competitively with the current standard model, while being simpler and more efficient.",
}

@misc{beltagy2020longformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bai2022graph,
      title={Graph Pre-training for AMR Parsing and Generation}, 
      author={Xuefeng Bai and Yulong Chen and Yue Zhang},
      year={2022},
      eprint={2203.07836},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{radford2021learning,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{N18-1101,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}

@inproceedings{Thorne18Fever,
    author = {Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
    title = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VERification}},
    booktitle = {NAACL-HLT},
    year = {2018}
}

@inproceedings{nie-etal-2020-adversarial,
    title = "Adversarial {NLI}: A New Benchmark for Natural Language Understanding",
    author = "Nie, Yixin  and
      Williams, Adina  and
      Dinan, Emily  and
      Bansal, Mohit  and
      Weston, Jason  and
      Kiela, Douwe",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.441",
    doi = "10.18653/v1/2020.acl-main.441",
    pages = "4885--4901",
    abstract = "We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.",
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@inproceedings{10.3115/1073083.1073135,
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
title = {BLEU: A Method for Automatic Evaluation of Machine Translation},
year = {2002},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073083.1073135},
doi = {10.3115/1073083.1073135},
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {311–318},
numpages = {8},
location = {Philadelphia, Pennsylvania},
series = {ACL '02}
}

@misc{zhang2020pegasus,
      title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization}, 
      author={Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
      year={2020},
      eprint={1912.08777},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{2020t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@misc{zhang2020bertscore,
      title={BERTScore: Evaluating Text Generation with BERT}, 
      author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
      year={2020},
      eprint={1904.09675},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{arslan2020benchmark,
      title={A Benchmark Dataset of Check-worthy Factual Claims}, 
      author={Fatma Arslan and Naeemul Hassan and Chengkai Li and Mark Tremayne},
      year={2020},
      eprint={2004.14425},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2023making,
      title={Making Large Language Models Perform Better in Knowledge Graph Completion}, 
      author={Yichi Zhang and Zhuo Chen and Wen Zhang and Huajun Chen},
      year={2023},
      eprint={2310.06671},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}