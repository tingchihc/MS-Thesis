Recently, a number of approaches have been proposed for generating summaries of multimodal content.~\citet{111_inproceedings} use input audio to generate textual summaries.~\citet{7926698} extract textual summaries from annotated and summarized videos.~\citet{article,zhu-etal-2018-msmo} propose an integrated approach which utilizes both textual and visual modalities as inputs and produce multimodal outputs summarizing text and video.~\citet{palaskar-etal-2019-multimodal} propose to generate abstractive summaries from open-domain videos. Despite this recent progress, existing models continue to struggle with capturing complementary information from multiple modalities. Unlike prior work in this space, we seek to generate textual summaries of evidence from multiple modalities for purposes of fact-checking.
